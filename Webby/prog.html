<!DOCTYPE html>
<html>
<head>
<style>
body {background-color: powderblue;}
h1 {color: red;}
<title> MyFon </title>
</head>
</style>
<body>
<h1> My_Coding_Portfolio</h1>
<p style="color: blue";> IQ has now been used as a measure of cognitive functioning for over a century. It has played a prominent part in developmental studies in many ways: as an index of normal development; for clinical diagnostics; as a descriptor of individual differences in cognitive ability; as explanation for differences in achievement or success in the world; as a predictor of future success as in school, training and occupational selection; and as an index for exploring causes of individual differences in cognitive ability. For example, it is argued that the current search for associations between molecular genetic variations and IQ ‚Äúwill transform both developmental psychology and developmental psychopathology‚Äù (Plomin & Rutter, Citation1998, p. 1223; see also Plomin, Citation2013). Likewise, Kovas, Haworth, Dale, and Plomin (Citation2007) say that their conclusions on the heritability of IQ ‚Äúhave far-reaching implicatio </p>
<p style="color: blue";> The validity of an IQ test‚Äîor what it actually measures‚Äîon the other hand, has always been a difficult subject. Since Galton in the 1880's (Citation1883) and Spearman (Citation1927) a little later, it has been widely assumed that the test measures ‚Äúintelligence,‚Äù commonly referred to as ‚Äúgeneral cognitive ability,‚Äù or g. The identity of that ability, however has never been agreed; its function has only been characterized metaphorically as a kind of pervasive cognitive energy, power or capacity, by analogy with physical strength. In consequence, measuring it has always been indirect, creating perpetual debate and controversy about the validity of the tests. This article is about such validity.
Validity of IQ Tests
 </p>
<p style="color: blue";> In scientific method, generally, we accept external, observable, differences as a valid measure of an unseen function when we can mechanistically relate differences in one to differences in the other (e.g., height of a column of mercury and blood pressure; white cell count and internal infection; erythrocyte sedimentation rate (ESR) and internal levels of inflammation; breath alcohol and level of consumption). Such measures are valid because they rely on detailed, and widely accepted, theoretical models of the functions in question. There is no such theory for cognitive ability nor, therefore, of the true nature of individual differences in cognitive functions. A number of analyses of the inter-correlations of aspects of test scores have produced theories of the statistical structure of score patterns, as in the Cattell-Horn-Carroll theory (see McGrew, Citation2 </p>
<p style="color: blue";> The alternative strategy has been to attempt to establish test validity indirectly, by comparison of a proposed measure with what is considered to be some other expression of intelligence. Galton (Citation1883) chose differences in social esteem; subsequently, scholastic performance and age-related differences were chosen. Typically, in constructing a test, cognitive problems or items thought to engage aspects of intelligence are devised for presentation to testees in trials. Those items on which differences in performance agree with differences in the criterion are put together to make up an intelligence test. There are many other technical aspects of test construction, but this remains the essential rationale. Thus, nearly all contemporary tests, such as the Stanford-Binet or the Woodcock-Johnson tests, rely on correlations of scores with those from other IQ or achievement tests as evidence of validity.
 </p>
<p style="color: blue";> However, the question of whether such procedure measures the fundamental cognitive ability (or g) assumed has continued to haunt the field. Measuring what we think is being measured is known as the construct validity of the test‚Äîsomething that cannot, by definition, be measured indirectly. Generally, a test is valid for measuring a function if (a) the function exists and is well characterized; and (b) variations in the function demonstrably cause variation in the measurement outcomes. Validation research should be directed at the latter, not merely at the relation between what are, in effect, assumed to be independent tests of that function (Borsboom, Mellenberg, & van Heerden, Citation2005).
 </p>
<p style="color: blue";> It is true to say that various attempts have been made to correlate test scores with some cortical/physiological measures in order to identify cerebral ‚Äúefficiency‚Äù as the core of intelligence. However, as Nisbett et al. (Citation2012), in their review for the American Psychological Association, point out, such studies have been inconsistent:
 </p>
<p style="color: blue";> Patterns of activation in response to various fluid reasoning tasks are diverse, and brain regions activated in response to ostensibly similar types of reasoning (inductive, deductive) appear to be closely associated with task content and context. The evidence is not consistent with the view that there is a unitary reasoning neural substrate. (p. 145)
 </p>
<p style="color: blue";> Haier et al. (Citation2009) likewise conclude after similar inconsistent results that ‚Äúidentifying a ‚Äòneuro-g‚Äô will be difficult‚Äù (p. 136). Associations have also been sought between various elementary tasks such as reaction time and IQ test scores. These have been difficult to interpret because the correlations are (a) small (leaving considerable variance, as well as true causes, unexplained) and (b) subject to a variety of other factors such as anxiety, motivation, experience with equipment, and training or experience of various kinds such as video game playing (e.g., Green & Bavelier, Citation2012).
 </p>
<p style="color: blue";> Accordingly, validation of IQ tests has continued to rely on correlation with other tests. That is, test validity has been forced to rely, not on calibration with known internal processes, but on correlation with other assumed expressions, or criteria, of intelligence. This is usually referred to as ‚Äúpredictive‚Äù or ‚Äúcriterion‚Äù validity. In almost all validity claims for IQ those criteria have been educational achievement, occupational level and job performance.
Predictive Validity of IQ
 </p>
<p style="color: blue";> It is undoubtedly true that moderate correlations between IQ and those criteria have been reported. For example, in their recent review Nisbett et al. (Citation2012) say ‚Äúthe measurement of intelligence‚Äîwhich has been done primarily by IQ tests‚Äîhas utilitarian value because it is a reasonably good predictor of grades at school, performance at work, and many other aspects of success in life‚Äù (p. 2). But how accurate and meaningful are such correlations?
 </p>
<p style="color: blue";> It is widely accepted that test scores predict school achievement moderately well, with correlations of around 0.5 (Mackintosh, Citation2011). The problem lies in the possible self-fulfilment of this prediction because the measures are not independent. Rather they are merely different versions of the same test. Since the first test designers such as Binet, Terman, and others, test items have been devised, either with an eye on the kinds of knowledge and reasoning taught to, and required from, children in schools, or from an attempt to match an impression of the cognitive processes required in schools. This matching is an intuitively-, rather than a theoretically-guided, process, even with nonverbal items such as those in the Raven's Matrices. As Carpenter, Just, and Shell (Citation1990) explained after examining John Raven's personal notes, ‚Äú ‚Ä¶ the description of the abilities that Raven intended to measure are primarily characteristics of the problems, not specifications of the requisit </p>
<p style="color: blue";> In other words, a correlation between IQ and school achievement may emerge because the test items demand the very kinds of (learned) linguistic and cognitive structures that are also the currency of schooling (Olson, Citation2005). As Thorndike and Hagen (Citation1969) explained, ‚ÄúFrom the very way in which the tests were assembled [such correlation] could hardly be otherwise‚Äù (p. 325). Evidence for this is that correlations between IQ and school achievement tests tend to increase with age (Sternberg, Grigorenko, & Bundy, Citation2001). And this is why parental drive and encouragement with their children's school learning improves the children's IQ, as numerous results confirm (Nisbett, Citation2009; Nisbett et al., Citation2012).
 </p>
<p style="color: blue";> Similar doubts arise around the use of occupational level, salary, and so on, as validatory criteria. Because school achievement is a strong determinant of level of entry to the job market, the frequently reported correlation (r ‚àº 0.5) between IQ and occupational level, and, therefore, income, may also be, at least partly, self-fulfilling (Neisser et al., Citation1996). Again, the measures may not be independent.
 </p>
<p style="color: blue";> The really critical issue, therefore, surrounds the question of whether IQ scores predict individual differences in the seemingly more independent measure of job performance. Indeed, correlation of IQ scores with job performance is regularly cited as underpinning the validity of IQ tests. Furnam (Citation2008) probably reflects most views when he says that ‚Äúthere is a large and compelling literature showing that intelligence is a good predictor of both job performance and training proficiency at work‚Äù (p. 204). In another strong commentary, Kuncel and Hezlett (Citation2010) refer to ‚Äúthis robust literature‚Äù as ‚Äúfacts‚Äù (p. 342). Ones, Viswesvaran, and Dilchert (Citation2005)  </p>
<p style="color: blue";> Unfortunately, nearly all authors merely offer uncritical citations of the primary sources in support of their statements (for exceptions see, for example, Wagner, Citation1994, and in the following sections). Instead of scrutiny of the true nature of the evidence, a conviction regarding a ‚Äúlarge and compelling literature‚Äù seems to have developed from a relatively small number of meta-analyses over a cumulative trail of secondary citations (Furnham, 2008, p. 204). It seems important, therefore, to take a closer look at the quality of data and method behind the much-cited associations between IQ and job performance, and how they have been interpreted. The aim, here, is not to do an exhaustive review of such studies, nor to offer a sweeping critique of meta-analyses, which hPredicting Job Performance From IQ Scores
 </p>
<p style="color: blue";> In contrast with the confidence found in secondary reports, even a cursory inspection of the primary sources shows that they are highly varied in terms of data quality and integrity, involving often-small samples and disparate measures usually obtained under difficult practical constraints in single companies or institutions. Their collective effect has mainly arisen from their combination in a few well-known meta-analyses. Hundreds of studies prior to the 1970s reported that correlations between IQ tests and job performance were low (approximately 0.2‚Äì0.3) and variable (reviewed by Ghiselli, Citation1973). These results were widely accepted as representative of the disparate contexts </p>
<p style="color: blue";> The Schmidt and Hunter approach (Citation1998), as first devised, seemed relatively straightforward. First, the results were collated from as many studies as were available. Then, the variance due to sampling error in the reported (observed) correlations was estimated. Then, the mean of the observed correlations was computed and corrected for measurement unreliability in the criterion (i.e., job performance) and for restriction of range in predictor and criterion measures. This produced the results now so widely cited in vindication of IQ test validity (Hunter, Schmidt, & Jackson, Citation1982; Schmidt & Hunter, Citation1977, Citation1998).
 </p>
<p style="color: blue";> Hunter and Hunter (Citation1984) first reported the application of these methods‚Äîusually referred to as ‚Äúvalidity generalization,‚Äù or VG‚Äîto the hundreds of studies reviewed by Ghiselli (Citation1973). In addition, they reported a further meta-analysis of 515 studies carried out by the U.S. Employment Service using the General Aptitude Test Battery (GATB). This produced corrected correlations in the range 0.5‚Äì0.6. Similar results have been reported from application of the same methods in more recent studies. For example, in meta-analyses of European and British studies, Salgado et al. (Citation2003) and Bertua, Anderson, and Salgado (Citation2005) found raw correlations between 0.12 and 0.34, depending on job category. However, all correlations virtually doubled under correction. Lang, Kersting, H√ºlsheger, and Lang (Citation2010) report similar results from meta-analysis of 50 studies in Germany.
Doubts About These Studies
 </p>
<p style="color: blue";> It is these corrected correlations from meta-analyses that are almost universally cited in favor of IQ as a predictor of job performance (and, by implication, that IQ really does measure something that can be called intelligence or general ability). But many doubts have been expressed regarding those methods, and results have been subject to continual criticism. Generally, meta-analyses are rarely straightforward and, at times, have been controversial. Although undoubtedly useful in many subject areas, as Murphy (Citation2003) says, they are often viewed with distaste because they mix good and bad studies, and encourage the drawing of strong conclusions from often-weak data. In the IQ-job perfoThe Many Surrogates of IQ Tests
 </p>
<p style="color: blue";> However well-intentioned, most studies have been done under difficult circumstances so that study design, including choice of test, has often been based on convenience rather than principles of empirical precision. Accordingly, a wide variety of vaguely mental tests has been adopted across individual studies, and incorporated into meta-analyses, on the assumption that they measure essentially the same thing (by implication ‚Äúgeneral intelligence‚Äù or g). Apart from the traditional psychometrically validated instruments (e.g., Wechsler's Adult Intelligence Scale, Raven's Progressive Matrices, or the U.S. Employment Service's General Aptitude Test Battery), studies have included working memory tests, reading tests, scholastic aptitude tests (SATS) and university admission tests, all taken in meta-analyses as surrogate measures of IQ. Sometimes, a ‚Äúgeneral‚Äù factor has been deduced as a composit </p>
<p style="color: blue";> Illustrative of the variety of tests used in meta-analysis are those listed in the European study of Salgado et al. (Citation2003). They include ‚Äú(a) Batteries: DAT, GATB, T2, ASDIC, Intelligence Structure Test (IST-70), Wilde Intelligence Test (WIT), GVK, PMA, and Aptitudes Mentales Primarias (AMPE); (b) g tests: Raven's Progressive Matrices, Cattell's Culture Fair Tests, Otis Employment Test, Alpha Test, Logique Intelligence, CERP, Domino, D-48, NIIP-33‚Äù (Salgado et al., Citation2003, p. 1070). This categorization implies that ‚Äúbatteries‚Äù and ‚Äúg tests‚Äù measure something different from each other‚Äîif so, what? More importantly, the studies using them cover a vast range of dates, some from the 1920s, while the majority are pre-1970s. These will not, of course, take any account of the ‚ÄúFlynn effect‚Äù‚ </p>
<p style="color: blue";> Further uncertainty is added by the high proportion of original studies involving men and women serving in the armed forces. These used a wide range of specialist and multi-purpose tests, such as the Armed Forces Qualification test, the Australian Army Intelligence Test, and the Armed Service Vocational Aptitude Battery. Sometimes, measures have been statistically reduced to a single component of variance, or primary factor, before meta-analysis (e.g., Olea & Ree, Citation1994). The usual justification for doing so is that any general factor condensed from inter-correlated scores can be assumed to represent g, and, therefore, that the tests are genuine tests of intelligence (even though a general factor typically covers only around 50% of the score variance). It is always a possibility, of course, that different correlates, even though resolving as a statistical ‚Äúcommo </p>
<p style="color: blue";> As Murphy (Citation2003) says, the assumption that these measures, with disparate properties, distributions, and so on, can be combined as if a single uniform variable can lead to serious problems in meta-analysis including ‚Äúlack of clarity in what population parameter is being estimated‚Äù (p. 31). Murphy and Newman (Citation2003) add that, ‚Äúif several hundred studies each claim to measure ability and performance, but they use wildly different measures of one or both constructs, the average ability-performance correlation across those studies might be hard to interpret‚Äù (p.414). Burke and Landis (Citation2003) also complain about the ‚Äúcavalier‚Äù treatment of construct issues in meta-analyses.
Job Performance?
 </p>
<p style="color: blue";> In contrast to the vast diversity of predictor tests, the measure of job performance has almost always consisted of supervisors‚Äô ratings. These, of course, should be reliable, valid, and free from bias of whatever source. Unfortunately, as with ability testing, the strict requirements are often overlooked (Guion, Citation2006). It turns out that there are a number of problems with such ratings (Woerh, Citation2011).
 </p>
<p style="color: blue";> The main problem is that supervisors tend to be subjective, and use inconsistent criteria, in making their judgments of performance. This is hardly surprising, given the difficulty of defining good or poor performance. As Gottfredson (Citation1991) noted, ‚ÄúOne need only ask a group of workers in the same job to suggest specific criterion measures for that job in order to appreciate how difficult it is to reach consensus about what constitutes good performance and how it can be measured fairly‚Äù (p. 76). In addition, a variety of systematic biases are evident: age effects and ‚Äúhalo‚Äù effects have been reported (e.g., Murphy & Balzer, Citation1986). Subjects‚Äô height (Judge & Cable, Citation2004); facial attractiveness (Hosoda, Stone-Romero, & </p>
<p style="color: blue";> Perhaps it is hardly surprising, therefore, that supervisor ratings have rather low correlations with more objective criteria such as work samples or work output (Bommer, Johnson, Rich, Podsakoff, & Mackenzie Citation1995; Cook, Citation2009; Heneman, Citation1986). Schmidt, Hunter, and Outerbridge (Citation1986) put it at virtually zero. In a study of salespersons, Vinchur, Schippmann, Switzer, and Roth (Citation1998) found that ‚Äúgeneral cognitive ability‚Äù showed a correlation of .40 with supervisor ratings but only .04 with objective sales. Roth, Bobko, and McFarland (Citation2005) found a mean observed correlation between work sample tests and measures of job performance (mostly supervisor ratings) of only 0.26, and a correlation between work sample tests and ‚Äúgeneral cognitive ability‚Äù of only 0.33. It is somewhat strange, therefore that Hunter (Citation1986) reported that IQ/GMA predicted work sample ratings even better than it predicted supervisor ratings suggesting, perh </p>
<p style="color: blue";> Another problem is the difficulty investigators have experienced in establishing reliabilities for supervisor ratings. Accurate reliabilities are needed, of course, in oThe Corrections
 </p>
<p style="color: blue";> In meta-analyses the reported correlation between IQ and job performance is a mean of observed correlations (usually weighted by sample size, if known). It could be that the low correlations from early studies are the true correlations for the general population of employees across their myriad jobs and contexts. Hunter and Schmidt (1977) argued, conversely, that the diverse correlations are artefacts of data collection. They devised a number of formulae for making corrections to them that have been refined over the years but remain essentially the same.
Corrections for Sampling Error
 </p>
<p style="color: blue";> First, sampling error arises because the observed (primary study) correlations are being estimated from sub-samples of the general population as well as sub-samples of the universe of jobs. The correlations, that is, will deviate from the (unknown) population correlation by an unknown degree, affecting the overall estimate as well as its confidence intervals. The mean of the observed correlations‚Äîas used in meta-analysis‚Äîwill also have an inflated variance. Therefore, the sampling error variance has to be subtracted from the overall variance to arrive at the variance for the true correlation and it's statistical significance. Estimates for all these values need to be computed from the data. In using their methods and assumptions Schmidt and Hunter (Citation1998) estimated that approximately 70% of the apparent variance consisted of sampling error variance.
 </p>
<p style="color: blue";> A number of issues surround corrections for sampling errors. The Schmidt and Hunter approach (Citation2003) assumes that all specific study samples are from essentially the same reference population with a single underlying IQ/job-performance correlation having close to zero variance. This assumption, together with the distribution of sampling errors, is used to indicate how close the average observed correlation is likely to be to the ‚Äútrue‚Äù correlation.
 </p>
<p style="color: blue";> However, this maneuver is based on the further assumption that the primary studies are random samples from the (hypothetical) general population. This cannot be checked in samples where a number of details are missing. Rather than being carefully planned as random designs, particular studies are conducted on an as available basis, as Murphy (Citation2003) puts it. After all, recruitment of participants is based on finding an employer willing to have employees tested and finding supervisors willing to rate them, which will be more likely to occur with some jobs than others. Hartigan and Wigdor (Citation1989) provide evidence of such bias. Moreover, effects of systematic moderator variables are rarely taken into account (Schmitt, Gooding, Noe, & Kirsch, Citation1984). These can only be eliminated through primary research with appropriate controls (Russell & Gilliland, Citation1995).
 </p>
<p style="color: blue";> When the corrections to sampling errors are done is also an issue. The estimated true mean correlation is computed as an average of observed correlations, as previously mentioned. Ideally, the sample means should be individually corrected for sampling error, measurement unreliability and range restriction before the averaging occurs; that is, meta-analysis should be done on the fully corrected samples. However, as most of that information is not available in the individual studies, the Schmidt and Hunter method (Citation2003) corrects for them after the averaging, which can introduce further inaccuracies including reduction of observed variance and exaggerated sampling error variance (Davar, Citation2004; Oswald & McCloy, Citation2003). Hartigan and Wigdor (Citation1989), in their meta-analysis of more recent studies, estimated sampliCorrections for Measurement Error
 </p>
<p style="color: blue";> The sample means may also deviate from the hypothetical true mean because of unreliability of measurement, or measurement error, in both ability test and job performance assessment. A correlation between IQ and job performance in a specific study may be depressed because of such error. That also needs to be corrected. The main effect of correcting for measurement error is to increase the observed correlations usually in proportion to the unreliability of the measure: the greater the unreliability the bigger the upward correction to the correlation.
 </p>
<p style="color: blue";> The desirable technique for measurement error correction consists of adjusting each coefficient included in the meta-analysis individually using reliability information provided for the specific predictor and criterion measures in the study report. In the most-used and reputable standardized tests reliability is well established and the attenuation can be corrected in advance of the meta-analysis. However, the reliabilities of the measures actually used in the meta-analyses in question were ‚Äúonly sporadically available‚Äù (Hunter & Schmidt, Citation1990, p. 79). They recommended basing them on the subset of the studies for which information happened to be available.
 </p>
<p style="color: blue";> Using that strategy, Schmidt and Hunter (Citation1977) arrived at a reliability of .60 for job performance. As Hartigan and Wigdor (Citation1989) explained, this figure ‚Äúhas met with some scepticism among industrial/organizational psychologists many of whom believe that the .60 value is too low‚Äù (p. 166). The overall effect of using the .60 value is to increase the estimate of the population correlation by 30%. This too has remained an area of controversy (Sackett, Citation2003).
 </p>
<p style="color: blue";> More generally, although correcting for measurement error seems straightforward and desirable, it is theoretically more complicated and may not always be consistent with psychometric principles (Murphy & DeShon, Citation2000). DeShon (Citation2003) says, ‚Äúthere are numerous theoretical reasons for urging caution when correcting the magnitude of the correlation coefficients for measurement error‚Äù, and it ‚Äúis of dubious merit in many situations‚Äù (p. 382). One of these is that, although correcting for measurement error will often increase the correlation coefficient, it also increases its standard error with larger confidence intervals not differentiating it from zero. Reliabil </p>
<p style="color: blue";> The statistical model used for meta-analysis and its corrections may also be an issue here. Correction of measurement error is based on a random effects model, but the unreliability of (in this case) supervisor ratings may stem in part from a number of systematic (i.e., non-random) effects (Murphy & DeShon, Citation2000). For example, different job contexts may involve different kinds of disagreement among raters about what should be measured or about how the rating scales should be used. Also, there may be systematic differences among testees related to, for example, gender, ethnic background and social class background and the effects of these on such variables as self-confidence and ability expression (see subsequent sections). A variety of studies indicate that ‚Äúmacrosocial differences in the distribution of economic goods are linked to microsocial processes of per </p>
<p style="color: blue";> Correcting for measurement error also has complex effects on the variances of the observed correlation coefficients. As implied above, corrections for measurement error made after, rather than before, averaging in meta-analyses, may exaggerate sampling error variance and reduces the variance of the estimated correlation. Much more statistical evaluation of the combination of known and unknown measurement unreliabilities is called for ‚Äúbefore this procedure could be recommended as general practice‚Äù (DeShon, Citation2003, p. 397).
 </p>
<p style="color: blue";> More generally, measurement error may also arise on account of the lack of construct validity (the proof that it is measuring the function intended). It is, of course, the acknowledged lack of construct validity in IQ testing that has led to such reliance on predictive validity in the first place. Lack of it, nevertheless, has implications for corrections for unreliability in meta-analyses. Schmidt and Hunter's approach (Citation1977) insists that correcting for measurement error provide an estimate of the ‚Äútrue‚Äù correlation between the underlying constructs. Borsboom and Mellenbergh (Citation2002), on the basis of classical test theory, have vehemently disagreed with this because it also assumes what it is trying to prove, namely the validity of that construct being revealed through the test-criterion correlation. As Burke and Landis (Citation2003) explain:  </p>
</body>
</html>
